{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoSeoAh/ML_2022/blob/main/Generalization_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 1) 사용자가 키워드(e.g. 기업명)를 입력하면 2) 바로 그때(현재)의 날짜와 함께 입력받아\n",
        "#2. 뉴스를 크롤링 하여\n",
        "#3. 해당 뉴스들이 포함된 데이터프레임을 반환\n",
        "\n",
        "keywords = input(\"데이터를 수집할 기업명을 입력하세요. :\")\n",
        "year = input(\"데이터를 수집할 연도(year)를 입력하세요. :\")\n",
        "month = input(\"데이터를 수집할 달(month)을 입력하세요. :\")\n",
        "start_day = input(\"데이터를 수집 기간 시작 날짜(day)를 입력하세요. :\")\n",
        "end_day = input(\"데이터를 수집 기간 마지막 날짜(day)를 입력하세요. :\")\n",
        "\n",
        "!pip install selenium==3.141\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "from selenium import webdriver as wd\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "options = wd.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "def get_article_info(driver, crawl_date, press_list, title_list, link_list, date_list, \n",
        "                     more_news_base_url=None, more_news=False):\n",
        "    more_news_url_list = []\n",
        "    while True:    \n",
        "        page_html_source = driver.page_source\n",
        "        url_soup = BeautifulSoup(page_html_source, 'lxml')\n",
        "        \n",
        "        more_news_infos = url_soup.select('a.news_more')\n",
        "        \n",
        "        if more_news:\n",
        "            for more_news_info in more_news_infos:\n",
        "                more_news_url = f\"{more_news_base_url}{more_news_info.get('href')}\"\n",
        "\n",
        "                more_news_url_list.append(more_news_url)\n",
        "\n",
        "        article_infos = url_soup.select(\"div.news_area\")\n",
        "        \n",
        "        if not article_infos:\n",
        "            break\n",
        "\n",
        "        for article_info in article_infos:\n",
        "            press_info = article_info.select_one(\"div.info_group > a.info.press\")\n",
        "             \n",
        "            if press_info is None:\n",
        "                press_info = article_info.select_one(\"div.info_group > span.info.press\")\n",
        "            article = article_info.select_one(\"a.news_tit\")\n",
        "            \n",
        "            press = press_info.text.replace(\"언론사 선정\", \"\")\n",
        "            title = article.get('title')\n",
        "            link = article.get('href')\n",
        "\n",
        "            press_list.append(press)\n",
        "            title_list.append(title)\n",
        "            link_list.append(link)\n",
        "            date_list.append(crawl_date)\n",
        "\n",
        "        time.sleep(2.0)\n",
        "                      \n",
        "                      \n",
        "        next_button_status = url_soup.select_one(\"a.btn_next\").get(\"aria-disabled\")\n",
        "        \n",
        "        if next_button_status == 'true':\n",
        "            break\n",
        "        \n",
        "        time.sleep(1.0)\n",
        "        next_page_btn = driver.find_element_by_css_selector(\"a.btn_next\").click()      \n",
        "    \n",
        "    return press_list, title_list, link_list, more_news_url_list\n",
        "    \n",
        "    \n",
        "\n",
        "def get_naver_news_info_from_selenium(driver_path, keyword, save_path, target_date, ds_de, sort=0, remove_duplicate=False):\n",
        "    crawl_date = f\"{target_date[:4]}.{target_date[4:6]}.{target_date[6:]}\"\n",
        "    driver = wd.Chrome(driver_path, options=options)\n",
        "\n",
        "    encoded_keyword = urllib.parse.quote(keyword)\n",
        "    url = f\"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort={sort}&photo=0&field=0&pd=3&ds={ds_de}&de={ds_de}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{target_date}to{target_date}&is_sug_officeid=0\"\n",
        "    \n",
        "    more_news_base_url = \"https://search.naver.com/search.naver\"\n",
        "\n",
        "    driver.implicitly_wait(0.8)\n",
        "\n",
        "    driver.get(url)\n",
        "    \n",
        "    press_list, title_list, link_list, date_list, more_news_url_list = [], [], [], [], []\n",
        "    \n",
        "    press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
        "                                                                             crawl_date=crawl_date, \n",
        "                                                                             press_list=press_list, \n",
        "                                                                             title_list=title_list, \n",
        "                                                                             link_list=link_list,\n",
        "                                                                             date_list=date_list,\n",
        "                                                                             more_news_base_url=more_news_base_url,\n",
        "                                                                             more_news=True)\n",
        "    driver.close()\n",
        "    \n",
        "    if len(more_news_url_list) > 0:\n",
        "        print(len(more_news_url_list))\n",
        "        more_news_url_list = list(set(more_news_url_list))\n",
        "        print(f\"->{len(more_news_url_list)}\")\n",
        "        for more_news_url in more_news_url_list:\n",
        "            driver = wd.Chrome(driver_path, options=options)\n",
        "            driver.implicitly_wait(0.8)\n",
        "            driver.get(more_news_url)\n",
        "            \n",
        "            press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
        "                                                                             crawl_date=crawl_date, \n",
        "                                                                             press_list=press_list, \n",
        "                                                                             title_list=title_list, \n",
        "                                                                             link_list=link_list,\n",
        "                                                                             date_list=date_list)\n",
        "            driver.close()\n",
        "\n",
        "    article_df = pd.DataFrame({\"날짜\": date_list, \"언론사\": press_list, \"제목\": title_list, \"링크\": link_list})\n",
        "    \n",
        "    print(f\"extract article num : {len(article_df)}\")\n",
        "    if remove_duplicate:\n",
        "        article_df = article_df.drop_duplicates(['링크'], keep='first')\n",
        "        print(f\"after remove duplicate -> {len(article_df)}\")\n",
        "    \n",
        "    article_df.to_excel(save_path, index=False)\n",
        "\n",
        "    from datetime import datetime\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    def crawl_news_data(driver_path, keyword, year, month, start_day, end_day, save_path_dir):\n",
        "        for day in tqdm(range(start_day, end_day+1)):\n",
        "            date_time_obj = datetime(year=year, month=month, day=day)\n",
        "            target_date = date_time_obj.strftime(\"%Y%m%d\")\n",
        "            ds_de = date_time_obj.strftime(\"%Y.%m.%d\")\n",
        "\n",
        "            get_naver_news_info_from_selenium(driver_path=driver_path, keyword=keyword, \n",
        "                                              save_path=f\"{save_path_dir}/{keyword}/{target_date}_{keyword}_.xlsx\", target_date=target_date, ds_de=ds_de, remove_duplicate=False)\n",
        "        \n",
        "    def bulk_keyword_crawler(driver_path, keywords, year, month, start_day, end_day, save_path_dir):\n",
        "        for keyword in keywords:\n",
        "            print(f\"start keyword - {keyword} crawling ...\")\n",
        "            crawl_news_data(driver_path=driver_path, keyword=keyword, year=year, month=month, start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)\n",
        "\n",
        "import os\n",
        "\n",
        "save_path_dir = \"/content/drive/MyDrive/ML\"\n",
        "driver_path = \"chromedriver\"\n",
        "\n",
        "#for keyword in keywords:\n",
        "#  os.makedirs(f\"{save_path_dir}/{keyword}\")\n",
        "\n",
        "bulk_keyword_crawler(driver_path=driver_path, keywords=keywords, year=year, month=month, \n",
        "                     start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "id": "i2lFUdKRNRiu",
        "outputId": "6d0bb87e-b972-4379-e83c-220a27baf79a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터를 수집할 기업명을 입력하세요. :기아\n",
            "데이터를 수집할 연도(year)를 입력하세요. :2022\n",
            "데이터를 수집할 달(month)을 입력하세요. :5\n",
            "데이터를 수집 기간 시작 날짜(day)를 입력하세요. :7\n",
            "데이터를 수집 기간 마지막 날짜(day)를 입력하세요. :8\n",
            "Requirement already satisfied: selenium==3.141 in /usr/local/lib/python3.7/dist-packages (3.141.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium==3.141) (1.24.3)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (100.0.4896.127-0ubuntu0.18.04.1).\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 50 not upgraded.\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-981ff227a952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;31m#  os.makedirs(f\"{save_path_dir}/{keyword}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m bulk_keyword_crawler(driver_path=driver_path, keywords=keywords, year=year, month=month, \n\u001b[0m\u001b[1;32m    159\u001b[0m                      start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bulk_keyword_crawler' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W58HF3OfWcAh",
        "outputId": "8785be07-ff73-4c95-abe0-f1de5a2a0c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start keyword - 기아 crawling ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "year, month = 2017, 6\n",
        "start_day, end_day = 16, 30\n",
        "keywords = ['기아', '삼성바이오로직스', '삼성전자', '삼성SDI', '카카오', '포스코', '현대차', 'LG화학', 'SK하이닉스']\n",
        "save_path_dir = \"/content/drive/MyDrive/ML\"\n",
        "driver_path = \"chromedriver\"\n",
        "\n",
        "#for keyword in keywords:\n",
        "#  os.makedirs(f\"{save_path_dir}/{keyword}\")\n",
        "\n",
        "bulk_keyword_crawler(driver_path=driver_path, keywords=keywords, year=year, month=month, \n",
        "                     start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM4euTdIWt79"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def merge_excel_files(file_path, file_format, save_path, save_format, columns=None):\n",
        "  merge_df = pd.DataFrame()\n",
        "  file_list = [f\"{file_path}/{file}\" for file in os.listdir(file_path) if file_format in file]\n",
        "\n",
        "  for file in file_list:\n",
        "    if file_format == \".xlsx\":\n",
        "      file_df = pd.read_excel(file)\n",
        "    else:\n",
        "      file_df = pd.read_csv(file)\n",
        "\n",
        "    if columns is None:\n",
        "      columns = file_df.columns\n",
        "    \n",
        "    temp_df = pd.DataFrame(file_df, columns=columns)\n",
        "\n",
        "    merge_df = merge_df.append(temp_df)\n",
        "\n",
        "  if save_format == \".xlsx\":\n",
        "    merge_df.to_excel(save_path, index=False)\n",
        "  else:\n",
        "    merge_df.to_csv(save_path, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for keyword in keywords:\n",
        "    merge_excel_files(file_path=f\"/content/drive/MyDrive/ML/{keyword}\", file_format=\".xlsx\",\n",
        "                      save_path=f\"/content/drive/MyDrive/ML/{keyword}/000000~000000_{keyword}_merged.xlsx\", save_format=\".xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zWzdvDZef6e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Generalization.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}