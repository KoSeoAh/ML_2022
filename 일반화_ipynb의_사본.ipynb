{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoSeoAh/ML_2022/blob/main/%EC%9D%BC%EB%B0%98%ED%99%94_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. 1) 사용자가 키워드(e.g. 기업명)를 입력하면 2) 바로 그때(현재)의 날짜와 함께 입력받아\n",
        "#2. 뉴스를 크롤링 하여\n",
        "#3. 해당 뉴스들이 포함된 데이터프레임을 반환\n",
        "\n",
        "keywords = input(\"데이터를 수집할 기업명을 입력하세요. :\")\n",
        "from datetime import datetime\n",
        "year = datetime.today().year\n",
        "month = datetime.today().month\n",
        "start_day = datetime.today().day - 1\n",
        "end_day = datetime.today().day\n",
        "print(keywords, year, month, start_day, end_day)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2lFUdKRNRiu",
        "outputId": "1937ea89-8164-44ec-afd8-1e27097d3417"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터를 수집할 기업명을 입력하세요. :현대차\n",
            "현대차 2022 5 7 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBIR33z9V3zz",
        "outputId": "ec66a0fc-adcf-41f4-becc-3beef66165b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium==3.141\n",
            "  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |▊                               | 20 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51 kB 20.2 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 61 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 71 kB 24.4 MB/s eta 0:00:01\r\u001b[K     |███                             | 81 kB 25.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 92 kB 27.6 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 102 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████                            | 112 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 122 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 133 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████                           | 143 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 153 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 163 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 174 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 184 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 194 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 204 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 215 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 225 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 235 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 245 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 256 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 266 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 276 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 286 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 296 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 307 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 317 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 327 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 337 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 348 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 358 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 368 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 378 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 389 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 399 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 409 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 419 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 430 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 440 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 450 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 460 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 471 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 481 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 491 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 501 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 512 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 522 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 532 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 542 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 552 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 563 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 573 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 583 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 593 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 604 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 614 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 624 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 634 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 645 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 655 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 665 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 675 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 686 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 696 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 706 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 716 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 727 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 737 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 747 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 757 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 768 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 778 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 788 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 798 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 808 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 819 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 829 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 839 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 849 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 860 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 870 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 880 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 890 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 901 kB 24.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 904 kB 24.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium==3.141) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,168 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,272 kB]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,954 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,734 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,001 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,498 kB]\n",
            "Fetched 12.9 MB in 6s (2,206 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  libnvidia-common-460 nsight-compute-2020.2.0\n",
            "Use 'apt autoremove' to remove them.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 88.4 MB of archives.\n",
            "After this operation, 297 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 100.0.4896.127-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 100.0.4896.127-0ubuntu0.18.04.1 [77.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 100.0.4896.127-0ubuntu0.18.04.1 [4,496 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 100.0.4896.127-0ubuntu0.18.04.1 [5,190 kB]\n",
            "Fetched 88.4 MB in 3s (28.1 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155203 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_100.0.4896.127-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_100.0.4896.127-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_100.0.4896.127-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_100.0.4896.127-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (100.0.4896.127-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium==3.141\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co-QIO7KMK6t",
        "outputId": "f973e9d5-fb5e-487e-fae1-3f404c20289f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFQ1jkFqWSvR"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver as wd\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import urllib\n",
        "\n",
        "options = wd.ChromeOptions()\n",
        "options.add_argument('--headless')        # Head-less 설정\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UjnTohRV5na"
      },
      "outputs": [],
      "source": [
        "def get_article_info(driver, crawl_date, press_list, title_list, link_list, date_list, \n",
        "                     more_news_base_url=None, more_news=False):\n",
        "    more_news_url_list = []\n",
        "    while True:    \n",
        "        page_html_source = driver.page_source\n",
        "        url_soup = BeautifulSoup(page_html_source, 'lxml')\n",
        "        \n",
        "        more_news_infos = url_soup.select('a.news_more')\n",
        "        \n",
        "        if more_news:\n",
        "            for more_news_info in more_news_infos:\n",
        "                more_news_url = f\"{more_news_base_url}{more_news_info.get('href')}\"\n",
        "\n",
        "                more_news_url_list.append(more_news_url)\n",
        "\n",
        "        article_infos = url_soup.select(\"div.news_area\")\n",
        "        \n",
        "        if not article_infos:\n",
        "            break\n",
        "\n",
        "        for article_info in article_infos:\n",
        "            press_info = article_info.select_one(\"div.info_group > a.info.press\")\n",
        "            \n",
        "            if press_info is None:\n",
        "                press_info = article_info.select_one(\"div.info_group > span.info.press\")\n",
        "            article = article_info.select_one(\"a.news_tit\")\n",
        "            \n",
        "            press = press_info.text.replace(\"언론사 선정\", \"\")\n",
        "            title = article.get('title')\n",
        "            link = article.get('href')\n",
        "\n",
        "            press_list.append(press)\n",
        "            title_list.append(title)\n",
        "            link_list.append(link)\n",
        "            date_list.append(crawl_date)\n",
        "\n",
        "        time.sleep(2.0)\n",
        "                      \n",
        "                      \n",
        "        next_button_status = url_soup.select_one(\"a.btn_next\").get(\"aria-disabled\")\n",
        "        \n",
        "        if next_button_status == 'true':\n",
        "            break\n",
        "        \n",
        "        time.sleep(1.0)\n",
        "        next_page_btn = driver.find_element_by_css_selector(\"a.btn_next\").click()      \n",
        "    \n",
        "    return press_list, title_list, link_list, more_news_url_list\n",
        "    \n",
        "    \n",
        "\n",
        "def get_naver_news_info_from_selenium(driver_path, keyword, save_path, target_date, ds_de, sort=0, remove_duplicate=False):\n",
        "    crawl_date = f\"{target_date[:4]}.{target_date[4:6]}.{target_date[6:]}\"\n",
        "    driver = wd.Chrome(driver_path, options=options)\n",
        "\n",
        "    encoded_keyword = urllib.parse.quote(keyword)\n",
        "    url = f\"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort={sort}&photo=0&field=0&pd=3&ds={ds_de}&de={ds_de}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{target_date}to{target_date}&is_sug_officeid=0\"\n",
        "    \n",
        "    more_news_base_url = \"https://search.naver.com/search.naver\"\n",
        "\n",
        "    driver.implicitly_wait(0.8)\n",
        "\n",
        "    driver.get(url)\n",
        "    \n",
        "    press_list, title_list, link_list, date_list, more_news_url_list = [], [], [], [], []\n",
        "    \n",
        "    press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
        "                                                                             crawl_date=crawl_date, \n",
        "                                                                             press_list=press_list, \n",
        "                                                                             title_list=title_list, \n",
        "                                                                             link_list=link_list,\n",
        "                                                                             date_list=date_list,\n",
        "                                                                             more_news_base_url=more_news_base_url,\n",
        "                                                                             more_news=True)\n",
        "    driver.close()\n",
        "    \n",
        "    if len(more_news_url_list) > 0:\n",
        "        print(len(more_news_url_list))\n",
        "        more_news_url_list = list(set(more_news_url_list))\n",
        "        print(f\"->{len(more_news_url_list)}\")\n",
        "        for more_news_url in more_news_url_list:\n",
        "            driver = wd.Chrome(driver_path, options=options)\n",
        "            driver.implicitly_wait(0.8)\n",
        "            driver.get(more_news_url)\n",
        "            \n",
        "            press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
        "                                                                             crawl_date=crawl_date, \n",
        "                                                                             press_list=press_list, \n",
        "                                                                             title_list=title_list, \n",
        "                                                                             link_list=link_list,\n",
        "                                                                             date_list=date_list)\n",
        "            driver.close()\n",
        "    article_df = pd.DataFrame({\"날짜\": date_list, \"언론사\": press_list, \"제목\": title_list, \"링크\": link_list})\n",
        "    \n",
        "    print(f\"extract article num : {len(article_df)}\")\n",
        "    if remove_duplicate:\n",
        "        article_df = article_df.drop_duplicates(['링크'], keep='first')\n",
        "        print(f\"after remove duplicate -> {len(article_df)}\")\n",
        "    \n",
        "    article_df.to_excel(save_path, index=False)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzsFkKn9WZ6o"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def crawl_news_data(driver_path, keyword, year, month, start_day, end_day, save_path_dir):\n",
        "    for day in tqdm(range(start_day, end_day+1)):\n",
        "        date_time_obj = datetime(year=year, month=month, day=day)\n",
        "        target_date = date_time_obj.strftime(\"%Y%m%d\")\n",
        "        ds_de = date_time_obj.strftime(\"%Y.%m.%d\")\n",
        "\n",
        "        get_naver_news_info_from_selenium(driver_path=driver_path, keyword=keyword, \n",
        "                                          save_path=f\"{save_path_dir}/{keyword}/{target_date}_{keyword}_.xlsx\", target_date=target_date, ds_de=ds_de, remove_duplicate=False)\n",
        "        \n",
        "def bulk_keyword_crawler(driver_path, keywords, year, month, start_day, end_day, save_path_dir):\n",
        "    for keyword in keywords:\n",
        "        print(f\"start keyword - {keyword} crawling ...\")\n",
        "        crawl_news_data(driver_path=driver_path, keyword=keyword, year=year, month=month, start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W58HF3OfWcAh",
        "outputId": "8785be07-ff73-4c95-abe0-f1de5a2a0c6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start keyword - 기아 crawling ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "year, month = 2017, 6\n",
        "start_day, end_day = 16, 30\n",
        "keywords = ['기아', '삼성바이오로직스', '삼성전자', '삼성SDI', '카카오', '포스코', '현대차', 'LG화학', 'SK하이닉스']\n",
        "save_path_dir = \"/content/drive/MyDrive/ML\"\n",
        "driver_path = \"chromedriver\"\n",
        "\n",
        "#for keyword in keywords:\n",
        "#  os.makedirs(f\"{save_path_dir}/{keyword}\")\n",
        "\n",
        "bulk_keyword_crawler(driver_path=driver_path, keywords=keywords, year=year, month=month, \n",
        "                     start_day=start_day, end_day=end_day, save_path_dir=save_path_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM4euTdIWt79"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "\n",
        "def merge_excel_files(file_path, file_format, save_path, save_format, columns=None):\n",
        "  merge_df = pd.DataFrame()\n",
        "  file_list = [f\"{file_path}/{file}\" for file in os.listdir(file_path) if file_format in file]\n",
        "\n",
        "  for file in file_list:\n",
        "    if file_format == \".xlsx\":\n",
        "      file_df = pd.read_excel(file)\n",
        "    else:\n",
        "      file_df = pd.read_csv(file)\n",
        "\n",
        "    if columns is None:\n",
        "      columns = file_df.columns\n",
        "    \n",
        "    temp_df = pd.DataFrame(file_df, columns=columns)\n",
        "\n",
        "    merge_df = merge_df.append(temp_df)\n",
        "\n",
        "  if save_format == \".xlsx\":\n",
        "    merge_df.to_excel(save_path, index=False)\n",
        "  else:\n",
        "    merge_df.to_csv(save_path, index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  for keyword in keywords:\n",
        "    merge_excel_files(file_path=f\"/content/drive/MyDrive/ML/{keyword}\", file_format=\".xlsx\",\n",
        "                      save_path=f\"/content/drive/MyDrive/ML/{keyword}/000000~000000_{keyword}_merged.xlsx\", save_format=\".xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zWzdvDZef6e"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "일반화.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}